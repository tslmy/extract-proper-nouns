#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import sys
import os.path
import re
import nltk
from nltk.tag import pos_tag
from nltk.tokenize import word_tokenize
# Parses arguments
if len(sys.argv) != 2:
    print ('Usage:', sys.argv[0], '<text file>')
    sys.exit(1)
filename = sys.argv[1]
if not os.path.exists(filename):
    print (filename + ': no such file')
    sys.exit(2)
# Extract proper nouns
with open (filename, "r") as file:
    text=file.read().replace('\n', ' ').strip()
# First, the punkt tokenizer divides our text in sentences.
# Each sentence is then tokenized and POS tagged.
sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')
exclusion_list = [
    re.compile('^(?:[A-Z]\.(\s|))+$'),
    re.compile('^[a-zA-Z]$')]
def yield_word(word):
    for pattern in exclusion_list:
        if pattern.match(word):
            return
    # if reached here, this word has passed all exclusion rules.
    print(word)
for sentence in sent_detector.tokenize(text):
    tokenizedSentence = word_tokenize(sentence)
    taggedSentence = pos_tag(tokenizedSentence)
    currentCandidate = []
    for word, pos in taggedSentence:
        if pos == 'NNP':
            currentCandidate.append(word)
        elif len(currentCandidate) > 0: # else, the noun phrase has ended.
            yield_word(' '.join(currentCandidate))
            currentCandidate = []
    if len(currentCandidate) > 0: # output the last nounphrase in the buffer
        yield_word(' '.join(currentCandidate))

